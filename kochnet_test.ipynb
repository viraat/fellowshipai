{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcdaHPvTl6k1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzQyO6MXl6lB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKzZO4aFl6lJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kel4S_AXl6lQ"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TO3cYOLl6nl"
   },
   "source": [
    "## Siamese networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94_wXkcJl6nn"
   },
   "outputs": [],
   "source": [
    "from dataloader import SiameseTestData_ImageFolder, SiameseTrainData_ImageFolder\n",
    "from networks import SiameseNet\n",
    "from losses import ContrastiveLoss\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from utils import AverageMeter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "- input size: the input size of the image for the model\n",
    "- learning rate: the learning rate for ADAM\n",
    "- epochs: total number of epochs to train\n",
    "- batch_size: size of the batch for the training data\n",
    "- num_workers: number of works for the dataloader\n",
    "- way: the n-way split for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uX05OgMLl6n5"
   },
   "outputs": [],
   "source": [
    "input_size = 105\n",
    "learning_rate = 1e-03\n",
    "epochs = 100\n",
    "sched_reset = 0\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "way = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = False\n",
    "pin_memory = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda = True\n",
    "    pin_memory = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data transforms and Load Data\n",
    "- Train\n",
    "    - Grayscale: ImageFolder returns RGB images, convert this to 1-channel\n",
    "    - Resize: Resize to input_size\n",
    "    - RandomRotation: Rotate the image about center\n",
    "    - ToTensor: Convert PIL image to tensor\n",
    "    - Normalize the images\n",
    "- Val: Same as train except for the random horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSinghKil6n7"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(10),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets using the generic ImageFolder class from pytorch.  \n",
    "The train, valid, test split is done in `train_test_split.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPOnZFyVl6oA",
    "outputId": "e3567e7d-9181-4c21-e3ac-771e8ad406df"
   },
   "outputs": [],
   "source": [
    "testset =  datasets.ImageFolder('./omniglot_data/changed/test', transform=data_transforms['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ImageFolder datasets to a Siamese form, i.e. which returns 2 imgs.  \n",
    "Classes are present in `dataloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_siamese = SiameseTestData_ImageFolder(testset, times=int(len(testset)/way))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE0ksk1wl6oi"
   },
   "outputs": [],
   "source": [
    "testloader =  torch.utils.data.DataLoader(test_siamese, batch_size=way, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Siamese Network according to the Koch et al. paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6WGnzHbl6ol"
   },
   "outputs": [],
   "source": [
    "class KochNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(KochNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # 1x105x105\n",
    "            nn.Conv2d(1, 64, kernel_size=10),\n",
    "            # 64x96x96\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # 64x48x48\n",
    "            nn.Conv2d(64, 128, kernel_size=7),\n",
    "            # 128x42x42\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # 128x21x21\n",
    "            nn.Conv2d(128, 128, kernel_size=4),\n",
    "            # 128x18x18\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # 128x9x9\n",
    "            nn.Conv2d(128, 256, kernel_size=4),\n",
    "            # 256x6x6\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.output = nn.Linear(4096, 1)\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dist = torch.abs(out1 - out2)\n",
    "        out = self.output(dist)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, epoch, device, print_freq=100):\n",
    "    batch_time = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    correct, wrong = 0, 0\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for batch_idx, (imgs1, imgs2) in enumerate(val_loader):\n",
    "            imgs1 = imgs1.to(device).float()\n",
    "            imgs2 = imgs2.to(device).float()\n",
    "            \n",
    "            output = model(imgs1, imgs2)\n",
    "            pred = np.argmax(output.cpu().numpy())\n",
    "            if pred == 0:\n",
    "                correct += 1\n",
    "            else: \n",
    "                wrong += 1\n",
    "           \n",
    "            acc = float(correct)/(correct+wrong)\n",
    "            accuracy.update(acc, correct+wrong)\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "    print('Test: [{0}][{1}/{2}]\\t'\n",
    "          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "          'Correct {correct} \\t Wrong {wrong}\\t'\n",
    "          'Accuracy {acc.val:.3f} ({acc.avg:.3f})\\t'.format(\n",
    "              epoch, batch_idx, len(val_loader), batch_time=batch_time,\n",
    "              correct=correct, wrong=wrong,\n",
    "              acc=accuracy))\n",
    "    return accuracy.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of KochNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (10): ReLU(inplace)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (output): Linear(in_features=4096, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = KochNet()\n",
    "final_model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KochNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (10): ReLU(inplace)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (output): Linear(in_features=4096, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.load_state_dict(torch.load('weights/good_weights/noflip_kochnet_72_82.pth'))\n",
    "final_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [1][658/659]\tTime 0.049 (0.049)\tCorrect 538 \t Wrong 121\tAccuracy 0.816 (0.833)\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8334758817308134"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(testloader, final_model, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
